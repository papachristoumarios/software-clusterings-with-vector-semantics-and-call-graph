%%%% Proceedings format for most of ACM conferences (with the exceptions listed below) and all ICPS volumes.
\documentclass[sigconf,review, anonymous]{acmart}
\usepackage{float}
%%%% As of March 2017, [siggraph] is no longer used. Please use sigconf (above) for SIGGRAPH conferences.

%%%% Proceedings format for SIGPLAN conferences 
% \documentclass[sigplan, anonymous, review]{acmart}

%%%% Proceedings format for SIGCHI conferences
% \documentclass[sigchi, review]{acmart}

%%%% To use the SIGCHI extended abstract template, please visit
% https://www.overleaf.com/read/zzzfqvkmrfzn

%
% defining the \BibTeX command - from Oren Patashnik's original BibTeX documentation.
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08emT\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    
% Rights management information. 
% This information is sent to you when you complete the rights form.
% These commands have SAMPLE values in them; it is your responsibility as an author to replace
% the commands and values with those provided to you when you complete the rights form.
%
% These commands are for a PROCEEDINGS abstract or paper.
\copyrightyear{2019}
\acmYear{2019}
\setcopyright{acmlicensed}

\acmConference[ESEC/FSE 2019]{The 27th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering}{26--30 August, 2019}{Tallinn, Estonia}

%
% These commands are for a JOURNAL article.
%\setcopyright{acmcopyright}
%\acmJournal{TOG}
%\acmYear{2018}\acmVolume{37}\acmNumber{4}\acmArticle{111}\acmMonth{8}
%\acmDOI{10.1145/1122445.1122456}

%
% Submission ID. 
% Use this when submitting an article to a sponsored event. You'll receive a unique submission ID from the organizers
% of the event, and this ID should be used as the parameter to this command.
%\acmSubmissionID{123-A56-BU3}

%
% The majority of ACM publications use numbered citations and references. If you are preparing content for an event
% sponsored by ACM SIGGRAPH, you must use the "author year" style of citations and references. Uncommenting
% the next command will enable that style.
%\citestyle{acmauthoryear}



%
% end of the preamble, start of the body of the document source.
\begin{document}

%
% The "title" command has an optional parameter, allowing the author to define a "short title" to be used in page headers.
\title{Software Clusterings with Vector Semantics and 
the Call Graph}

%
% The "author" command and its associated commands are used to define the authors and their affiliations.
% Of note is the shared affiliation of the first two authors, and the "authornote" and "authornotemark" commands
% used to denote shared contribution to the research.
\author{Marios Papachristou}

\email{papachristoumarios@gmail.com}
\affiliation{%
  \institution{National Technical University of Athens}
  \streetaddress{Zografou Campus}
  \city{Athens}
  \state{Attica}
  \postcode{157 73}
}

\renewcommand{\shortauthors}{Papachristou}

\begin{abstract}

\noindent \textbf{Purpose -- } Propose a method to determine a software's modules without 
knowledge of its architectural structure, and empirically validate the method's performance.

\noindent \textbf{Method -- } Cluster files by combining document embeddings, generated with the  \emph{Doc2Vec} algorithm, and the \emph{call graph}, provided by Static Graph Analyzers to an augmented graph.
Use of the \emph{Louvain Algorithm} to iteratively exhibit its community structure and propose a module--level clustering. 


\noindent \textbf{Results --} Our method performs better in terms of stability, authoritativeness 
and extremity over other state-of-the-art clustering methods proposed in literature and is able to decently 
recover the ground truth clustering of the Linux Kernel. 

\noindent \textbf{Conclusions --} Semantic information from vector semantics as well as the call graph can produce 
accurate results for software clusterings of large systems.

\end{abstract}

%
% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
% Please copy and paste the code instead of the example below.
%
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010147.10010257.10010258.10010260.10003697</concept_id>
<concept_desc>Computing methodologies~Cluster analysis</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010257.10010321</concept_id>
<concept_desc>Computing methodologies~Machine learning algorithms</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10011007.10010940.10010971.10010972</concept_id>
<concept_desc>Software and its engineering~Software architectures</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Cluster analysis}
\ccsdesc[500]{Computing methodologies~Machine learning algorithms}
\ccsdesc[500]{Software and its engineering~Software architectures}
%
% Keywords. The author(s) should pick words that accurately describe the work being
% presented. Separate the keywords with commas.
\keywords{document embeddings, doc2vec, linux kernel, natural language processing, software architecture recovery, software clustering, static graph analysis, vector semantics}



\maketitle

\section{Introduction}



%Modern software systems usually span hundreds of thousands of source code lines (SLOC) 
%since the inherent complexity of modern applications that computers have to solve has 
%exponentially increased. 

In modern software systems, the need for adoption of modular software architectural schemata 
becomes inevitable for robust maintainability of the overall software system.
Since the architecture of a software system is the most fundamental realization of a 
software system, when there is no description of the architecture of it, software 
architects usually make attempts to recover it. In the past, there have been attempts to 
identify the main components of a software system using hierarchical clustering 
algorithms \cite{maqbool_overview, limbo} mainly by clustering modules into subsystems, given 
its source code.    
The possibilities offered by machine learning techniques and practices in the Natural Language Processing (NLP) 
can also be applied on codebases as corpora of text, thus enabling 
the extraction of useful information for the semantics of the source codebase 
inaugurating new modes of reviewing large amounts of source code.
An example is demonstrated through the Deep Code Search Project \cite{deepcodesearch} .

This paper aims to propose software clusterings using a combination of vector semantics on the source code provided 
by \emph{Doc2Vec} as well as the existing structure of 
the codebase via the call graph in order to form communities and arrive at a module-level 
architecture viewed from the call graph perspective. 
We will study the Linux codebase and we will compare it with ground truth in using the MoJo measure \cite{mojo} as well as 
compare it with other state-of-the-art and baseline clustering algorithms  in terms of \emph{stability}, \emph{authoritativeness} and \emph{extremity}
\cite{maqbool_overview, evaluation}.

\section{Method} 

\begin{table*}
  \caption{Experimental Results for Linux 4.21 Codebase}
    \label{tab:evaluation}
    \begin{tabular}{lrrrrrr}
    \toprule
    Algorithm & Number of Clusters & Cluster Size Range & Avg. Cluster & Standard Deviation & Median Cluster & MoJo Distance \\
    \midrule
    ACDC \cite{acdc} & 9055 & 1 -- 4245 & 5 & 46 & 2 & 33694\\
    Average Linkage \cite{average} & 21 & 1--3406 & 163 & 725 & 1 & 2092 \\
    Complete Linkage \cite{complete} & 21 & 1--1529 & 163 & 412 & 19.0 & 1710 \\
    LIMBO \cite{limbo} & 21 & 50--1810 & 163 & 375 & 50  & 1482 \\

    Ward Linkage \cite{ward} & 21 & 21--948 & 163 & 223 & 70 & 1138 \\
        
    SADE & 10 (\pm 2)  & 2 (\pm 0) -132 (\pm 13) & 64 (\pm 4) & 40 (\pm 4) & 65 (\pm 10) & 243 (\pm 1)  \\
    SADE (Directed) & 5 (\pm 2) & 1 (\pm 1) - 614 (\pm 1) & 141 (\pm 39) & 253 (\pm 25) & 2 (\pm 0.3)  & 237 (\pm 2) \\
    \midrule
    Ground Truth & 21 & 1--1348 & 163 & 341 & 11.0 & -- \\
    \bottomrule
  \end{tabular}
\end{table*}

\paragraph{Embeddings} 

For the prepossessing stage use the NLP package spaCy \cite{spacy} which provided us with very useful tools for the extraction. 
After prepossessing, we use Gensim \cite{gensim} to train the Doc2Vec model. 
First of all, we tokenize the code and remove the stop-words. 
After that, for each token, we split it into its constituent parts using dynamic programming \cite{wordninja} and lemmatize 
each individual sub-token. 
For example, the method \texttt{\_\_zone\_seqlock\_init} corresponds to \texttt{zone}, \texttt{seqlock} and \texttt{init}, 
\texttt{inprogress} is split into \texttt{in} and \texttt{progress} and \texttt{literals} becomes \texttt{literal}.


\paragraph{Call Graph Generation} 
The codebase is processed by a Static Graph Analyzer. 
In our approach, focusing on C projects, we use \emph{CScout} \cite{cscout} in order to generate the directed call graph through function calls between the files. 
Each source code file is assigned to a module. 
Modules can be user defined or automatically generated by their respective directories at a desired directory tree depth. 

\paragraph{Combining semantics and the Call Graph} 
The input to the clustering procedure
is the call graph with cosine similarities of the various modules as weights, thus forming a
weighted directed graph. Then we run the Louvain Clustering Algorithm \cite{louvain} in
order to obtain the software clustering via maximizing the modularity function with a greedy approach. 
In case we want to use edge directionality we do a bipartite transformation, detect communities and
merge the results with a disjoint-set data structure.
    

\paragraph{Implementation}  
Our method is implemented in the Python Language using spaCy\cite{spacy}, Gensim \cite{gensim} and NetworkX \cite{nx}.

\paragraph{Decision Rationale for Evaluation System} 
We have chosen to use Linux as a codebase to evaluate our method on 
since it is a large and complex system with 27 years of continuous development and spanning 20.3 millions source lines of code (SLOC) at the time of writing.  
Moreover, it is easy to find ground truth system due to its clear directory structure and construct test-cases for
evaluations. 
Finally, it was also used in related studies \cite{acdc, evaluation} as a reference system for evaluation. 

\paragraph{Experimental Setting} Our method was run on Linux 4.21,
consisting of 20.3 million SLOC against Average-Linkage \cite{average}, Complete-Linkage \cite{complete} 
and Ward-Linkage \cite{ward} using the same document embeddings
as well as ACDC with structural information \cite{acdc} and LIMBO \cite{limbo} with $B = 100$, $S = \infty$ and Bag-of-Words features. The results are presented in Table \ref{tab:evaluation}. 
As ground truth we have used the first level directories as a target clustering and as input we have considered 
the modules of the one-top directories. 
For example, the source code file \texttt{drivers/net/ieee802154\-/mcr20a.c} has a ground truth value of \texttt{drivers} 
and it is considered under the same module as every \texttt{.c} and \texttt{.h} file under \texttt{drivers/net/ieee802154}. 
As a clustering distance measure we have used the MoJo distance measure, as proposed in reference \cite{mojo}. 
Since Louvain Community Detection produces  different (but very similar) results every time it runs, we ran the simulations 
multiple times and averaged the results. 

\section{Results, Evaluation \& Discussion}

In Table \ref{tab:evaluation}, we present the results of the clusterings ordered in increasing MoJo distance 
with respect to the Ground Truth. 
We have included the number of clusters produced, the cluster size range, the average cluster size the standard
deviation in the cluster sizes, the median cluster and the MoJo distances with respect to the ground truth. 
The Ground Truth clustering is also present in the table. 
Our quality metrics for the clusterings are 
\emph{extremity}, \emph{authoritativeness} and \emph{stability} as proposed in reference \cite{evaluation}.

Firstly, our approach produces balanced clusterings thus demonstrating low \emph{extremity} compared to the other clusterings, 
producing a number of clusters that was close to the Ground Truth with smaller standard deviation in the cluster sizes than 
the other clustering methods, without knowing the number of clusters \emph{a priori}. 
Besides this, our approach also gives balanced clusters with respect to \emph{Median Cluster} criterion. 
Note that ACDC, which takes into account only the structural properties of the files, produced a high 
granularity clustering that failed to catch the structure of Linux Codebase. 
Moreover, in the \emph{authoritativeness} criterion, our approach outperforms the other clusterings to a very up to 10 times 
in terms of MoJo distance being very close (smaller is better) to the Ground Truth, with a very small standard deviation. 
This also suggests great \emph{stability} in the resulting clusterings. 
Note that the other clustering methods we compared against ours gave disappointing MoJo distances even with their cluster
number defined equal to the one of the Ground Truth. 

\section{Related Work}

Onaiza Maqbool and Haroon A. Babri \cite{maqbool_overview} present an
overview of the various approaches taken towards to hierarchical clustering algorithms
for software systems. In their study, they compare the various clustering algorithms
such as ACDC, LIMBO and other Traditional Clustering Algorithms such as Single
Linkage, Complete Linkage and Weighted Linkage presenting their efficiency regarding 
multiple feature sets. 
Added to this, a \emph{semantics-based architectural view} of the system, as discussed in reference
\cite{large_study} reveals significant aspects of a software system and its change over 
time, which suggests that semantic-based approached should be followed for gaining better 
understandings of a software system. Their suggestion in \cite{large_study} is a \emph{term frequency--inverse 
document frequency} (TF-IDF) model serving as a similarity measure between source code 
entities. 
Work in Software Clustering Algorithms (SCA) has been extensively done by Tzerpos in
references \cite{mojo}, \cite{stability}, \cite{acdc} and \cite{limbo} with the ACDC
and LIMBO clustering algorithms. 

In particular, the ACDC algorithm leans toward to software components comprehension 
based on subsystem patterns. Their approach considers an initial structure of the system, 
without taking into account semantics, and tries to build comprehensive clusterings of the given ground structure. 
The authors also conducted a study on an older version of  Linux. 
Moreover, Andritsos and Tzerpos in reference \cite{limbo} present an Information-Theoretic 
approach of SCA by developing LIMBO which clusters modules upon inserting their Distributional Cluster Features 
to a B+-tree variant and then applying the \emph{Agglomerative Information Bottleneck} algorithm, merging the 
clusters with the minimum mutual information loss together. 


\section{Conclusions} 

This paper is set out to show how we can use a combination of vector semantics
and information from the call graph in order to produce meaningful clusterings of a software system.
We conclude that we have outperformed state-of-the-art software clusterings and conventional
agglomerative clustering algorithms 
in terms of \emph{extremity}, \emph{authoritativeness} and \emph{stability} 
without even knowing the number of clusters of the Ground Truth. This evidence strongly
supports the further usage of vector semantics and the call graph for architecture recovery.
Finally, further work is encouraged in several areas such as 
integration with more static analyzers and use of the project in order to recover layered architectures.


\newpage
\bibliographystyle{ACM-Reference-Format}
\bibliography{references}



\end{document}
