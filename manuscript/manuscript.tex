\documentclass[sigconf, screen]{acmart}
\usepackage{float}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08emT\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    
\setcopyright{rightsretained}
\acmPrice{15.00}
\acmDOI{10.1145/3338906.3342483}
\acmYear{2019}
\copyrightyear{2019}
\acmISBN{978-1-4503-5572-8/19/08}
\acmConference[ESEC/FSE '19]{Proceedings of the 27th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering}{August 26--30, 2019}{Tallinn, Estonia}
\acmBooktitle{Proceedings of the 27th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE '19), August 26--30, 2019, Tallinn, Estonia}


\begin{document}

%
% The "title" command has an optional parameter, allowing the author to define a "short title" to be used in page headers.
\title{Software Clusterings with Vector Semantics and 
the Call Graph}

%
% The "author" command and its associated commands are used to define the authors and their affiliations.
% Of note is the shared affiliation of the first two authors, and the "authornote" and "authornotemark" commands
% used to denote shared contribution to the research.
\author{Marios Papachristou}

\email{papachristoumarios@gmail.com}
\additionalaffiliation{%
  \institution{Business Analytics Lab at Athens University of Economics and Business}
  \streetaddress{Patision Campus}
  \city{Athens}
  \state{Attica}
  \postcode{104 34}
}
\affiliation{%
  \institution{National Technical University of Athens}
  \streetaddress{Zografou Campus}
  \city{Athens}
  \state{Attica}
  \country{Greece}
  \postcode{157 73}
}
\titlenote{The research described has been carried out as part of the CROSSMINER Project, which has received funding from the European Union's Horizon 2020 Research and Innovation Programme under grant agreement No. 732223}

\renewcommand{\shortauthors}{Papachristou}

\begin{abstract}

In this paper, we propose a novel method to determine a software's modules without knowledge of its architectural structure, and empirically validate the method's performance. We cluster files by combining document embeddings, generated with the  \emph{Doc2Vec} algorithm, and the \emph{call graph}, provided by Static Graph Analyzers to an augmented graph. We use the \emph{Louvain Algorithm} to determine its community structure and propose a module--level clustering. 
Our method performs better in terms of stability, authoritativeness 
, and extremity over other state-of-the-art clustering methods proposed in the literature and is able to decently 
recover the ground truth clustering of the Linux Kernel. 
Finally, we conclude that semantic information from vector semantics as well as the call graph can produce 
accurate results for software clusterings of large systems.

\end{abstract}

%
% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
% Please copy and paste the code instead of the example below.
%
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010147.10010257.10010258.10010260.10003697</concept_id>
<concept_desc>Computing methodologies~Cluster analysis</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010257.10010321</concept_id>
<concept_desc>Computing methodologies~Machine learning algorithms</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10011007.10010940.10010971.10010972</concept_id>
<concept_desc>Software and its engineering~Software architectures</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Cluster analysis}
\ccsdesc[500]{Computing methodologies~Machine learning algorithms}
\ccsdesc[500]{Software and its engineering~Software architectures}
%
% Keywords. The author(s) should pick words that accurately describe the work being
% presented. Separate the keywords with commas.
\keywords{document embeddings, doc2vec, linux kernel, natural language processing, software architecture recovery, software clustering, static graph analysis, vector semantics}



\maketitle

\section{Introduction}



%Modern software systems usually span hundreds of thousands of source code lines (\textsc{sloc}) 
%since the inherent complexity of modern applications that computers have to solve has 
%exponentially increased. 

In modern software systems, the need for adoption of modular software architectural schemata 
becomes inevitable for robust maintainability of their codebases.
Since the architecture of a software system is the most fundamental realization of the 
software system, when there is no description of the architecture of it, software 
architects make attempts to recover it. In the past, methods have been proposed to 
identify the main components of a software system using hierarchical clustering 
algorithms \cite{maqbool_overview, limbo} mainly by clustering modules into subsystems, given 
using the source code.    
Recently, the possibilities offered by machine learning techniques and practices in the Natural Language Processing (\textsc{nlp}) 
can also be applied on codebases as corpora of text, thus enabling 
the extraction of useful information for the semantics of the source code 
inaugurating new modes of reviewing the codebase. 

Our motivation, through this paper, is to use new features, namely vector semantics, and structural information via the call graph in order to perform community detection and arrive at module-level clusterings. 
We will study the Linux codebase and we will compare it with ground truth  using the MoJo measure \cite{mojo} as well as 
compare it with other state-of-the-art and baseline clustering algorithms  in terms of \emph{stability}, \emph{authoritativeness} and \emph{extremity}
\cite{maqbool_overview, evaluation}.

\section{Related Work}

Onaiza Maqbool and Haroon A. Babri \cite{maqbool_overview} present an
overview of the various approaches taken towards hierarchical clustering algorithms
for software systems. In their study, they compare many clustering algorithms
such as \textsc{acdc}, \textsc{limbo} and other Traditional Clustering Algorithms such as Single
Linkage, Complete Linkage and Weighted Linkage presenting their efficiency regarding 
multiple feature sets. 
Added to this, a \emph{semantics-based architectural view} of the system, as discussed in reference
\cite{large_study} reveals significant aspects of a software system and its change over 
time, which suggests that semantic-based approaches should be followed for gaining better understandings of a software system. 

Work in Software Clustering Algorithms (\textsc{sca}) has been extensively done by many authors in
references \cite{mojo}, \cite{stability}, \cite{acdc} and \cite{limbo} with the \textsc{acdc}
and \textsc{limbo} clustering algorithms. 
In particular, the \textsc{acdc} algorithm leans toward to software components comprehension 
based on subsystem patterns. Their approach considers an initial structure of the system, 
without taking into account semantics, and tries to build comprehensive clusterings of the given ground structure. 
The authors also conducted a study on an older version of  Linux. 
Moreover, Andritsos and Tzerpos in reference \cite{limbo} present an Information-Theoretic 
approach of \textsc{sca} by developing \textsc{limbo} which clusters modules upon inserting their Distributional Cluster Features 
to a B+-tree variant and then applying the \emph{Agglomerative Information Bottleneck} algorithm.


\section{Method} 


\begin{table*}
  \caption{Experimental Results for Linux 4.21}
  \small
    \label{tab:evaluation}
    \begin{tabular}{lrrrrrrr}
    \toprule
    Algorithm & Feature Dimension & \# Clusters & Size Range & $\bar x$ & $\sigma$ & Median & MoJo Distance \\
    \midrule
    \textsc{acdc} \cite{acdc} & -- & 9055 & 1 -- 4245 & 5 & 46 & 2 & 33694\\
    Average Linkage \cite{average} & \emph{300} & \emph{21} & 1--3406 & 163 & 725 & 1 & 2092 \\
    Complete Linkage \cite{complete} & \emph{300} & \emph{21} & 1--1529 & 163 & 412 & 19 & 1710 \\
    \textsc{limbo} ($B=100, \; S = \infty$) \cite{limbo} & 12317 &\emph{21} & 50--1810 & 163 & 375 & 50  & 1482 \\

    Ward Linkage \cite{ward} & \emph{300} & \emph{21} & 21--948 & 163 & 223 & 70 & 1138 \\
        
    \textsc{sade} & \emph{300} & 10 (\pm 2)  & 2 (\pm 0) -132 (\pm 13) & 64 (\pm 4) & 40 (\pm 4) & 65 (\pm 10) & 243 (\pm 1)  \\
    \textsc{sade} (Directed) & \emph{300} & 5 (\pm 2) & 1 (\pm 1) - 614 (\pm 1) & 141 (\pm 39) & 253 (\pm 25) & 2 (\pm 0.3)  & 237 (\pm 2) \\
    \midrule
    Ground Truth & -- & 21 & 1--1348 & 163 & 341 & 11.0 & -- \\
    \bottomrule
  \end{tabular}
\end{table*}

The codebase is initially preprocessed and the resulting data is fed into a Skip-Gram model for the initial components, which correspond to nodes in the call graph. Next, we add weights between the nodes with the normalized similarity between them. Finally, we run the Louvain algorithm to obtain a software clustering. The overall process is described below.

First of all, we tokenize the code and remove the stop-words. 
After that, for each token, we split it into its constituent parts using dynamic programming \cite{wordninja} and lemmatize 
each individual sub-token via using the \textsc{nlp} package spaCy \cite{spacy}. 
For example, the method \texttt{\_\_zone\_seqlock\_init} corresponds to \texttt{zone}, \texttt{seqlock} and \texttt{init}, 
\texttt{inprogress} is split into \texttt{in} and \texttt{progress} and \texttt{literals} becomes \texttt{literal}.
We, then, train a Doc2Vec model using Gensim \cite{gensim}. 
The codebase is also processed by a Static Graph Analyzer. 
In our approach, focusing on C projects, we use \emph{CScout} \cite{cscout} in order to generate the directed call graph through function calls between the files. 
Each source code file is assigned to a module. 
Modules can be user-defined or automatically generated by their respective directories at a desired directory tree depth. 
The input to the clustering procedure
is the call graph with cosine similarities of the modules as edge weights.
Then we run the Louvain Clustering Algorithm \cite{louvain} in
order to obtain the software clustering via maximizing the modularity function with a greedy approach. 
In case we want to consider edge directionality, we do a bipartite transformation, detect communities and
merge the results with a disjoint-set data structure as stated in reference \cite{malliaros}.

Our method is implemented in the Python Language using spaCy\cite{spacy}, Gensim \cite{gensim} and NetworkX \cite{nx} and is named \textsc{sade}. The source code and data are available in \cite{source_code} and \cite{call_graph}. 


We have chosen to use Linux as a codebase to evaluate our method on since it is a large and complex system with 27 years of continuous development and spanning 20.3 millions source lines of code (\textsc{sloc}) at the time of writing.  
Moreover, its ground truth is is easy to due to the clear directory structure and construct test-cases for
evaluations. 
Finally, it was also used in related studies \cite{acdc, evaluation} as a reference system for evaluation.  

 Our method was tested on Linux 4.21,
consisting of 20.3 million \textsc{sloc} against Average-Linkage \cite{average}, Complete-Linkage \cite{complete} 
and Ward-Linkage \cite{ward} using the same document embeddings
as well as \textsc{acdc} with structural information \cite{acdc} and \textsc{limbo} \cite{limbo} with Bag-of-Words features. The results are presented in Table \ref{tab:evaluation} and user-defined parameters are formatted in italics. 
As ground truth, we have used the first level directories as a target clustering and as input, we have considered 
the modules of the one-top directories. 
For example, the source code file \texttt{drivers/net/ieee802154\-/mcr20a.c} has a ground truth value of \texttt{drivers} 
and it is considered under the same module as every \texttt{.c} and \texttt{.h} file under \texttt{drivers/net/ieee802154}. Since Louvain Community Detection produces different (but very similar) results from every time it runs, we ran the simulations 
multiple times and averaged the results. 
The experimentation with a large and complex system may constitute a threat to our findings' validity since the results are not general.  


\section{Results, Evaluation \& Discussion}

In Table \ref{tab:evaluation}, we present the results of the clusterings ordered in increasing MoJo distance 
with respect to the ground truth. 
We have included the number of clusters produced, the cluster size range, the average cluster size the standard
deviation in the cluster sizes, the median cluster and the MoJo distances with respect to the ground truth. 
The ground truth clustering is also present in the table. 
Our quality metrics for the clusterings are 
\emph{extremity}, \emph{authoritativeness} and \emph{stability} as proposed in reference \cite{evaluation}.

Our approach produces balanced clusterings thus demonstrating low \emph{extremity} compared to the other clusterings, 
producing a number of clusters that is close to the ground truth with smaller standard deviation in the cluster sizes than 
the other clustering methods, without knowing the number of clusters \emph{a priori}. 
Besides this, it gives balanced clusters with respect to \emph{Median Cluster} criterion. 
Note that \textsc{acdc}, which takes into account only the structural properties of the files, produced a high 
granularity clustering that failed to catch the structure of Linux. 
In the \emph{authoritativeness} criterion, our approach outperforms the other clusterings
in terms of MoJo distance being very close (smaller is better) to the ground truth, with small standard deviation, posing great \emph{stability} in the resulting clusterings. 
Closing, the other clustering methods gave disappointing MoJo distances even with a fixed number of clusters. 



\section{Conclusions} 

This paper is set out to show how we can use a combination of vector semantics
and information from the call graph in order to produce meaningful clusterings of a software system.
We have outperformed state-of-the-art software clusterings and conventional
agglomerative clustering algorithms 
in terms of \emph{extremity}, \emph{authoritativeness} and \emph{stability} 
without even knowing the number of clusters of the ground truth. This evidence 
supports the further usage of vector semantics and the call graph for architecture recovery.
Finally, the further integration with static analyzers and the development of evaluation policies with users should be taken into account, especially when dealing with old codebases lacking technical documentation.

\newpage
\bibliographystyle{ACM-Reference-Format}
\bibliography{references}



\end{document}

