%%%% Proceedings format for most of ACM conferences (with the exceptions listed below) and all ICPS volumes.
\documentclass[sigconf,review, anonymous]{acmart}
\usepackage{float}
%%%% As of March 2017, [siggraph] is no longer used. Please use sigconf (above) for SIGGRAPH conferences.

%%%% Proceedings format for SIGPLAN conferences 
% \documentclass[sigplan, anonymous, review]{acmart}

%%%% Proceedings format for SIGCHI conferences
% \documentclass[sigchi, review]{acmart}

%%%% To use the SIGCHI extended abstract template, please visit
% https://www.overleaf.com/read/zzzfqvkmrfzn

%
% defining the \BibTeX command - from Oren Patashnik's original BibTeX documentation.
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08emT\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    
% Rights management information. 
% This information is sent to you when you complete the rights form.
% These commands have SAMPLE values in them; it is your responsibility as an author to replace
% the commands and values with those provided to you when you complete the rights form.
%
% These commands are for a PROCEEDINGS abstract or paper.
\copyrightyear{2019}
\acmYear{2019}
\setcopyright{acmlicensed}

\acmConference[ESEC/FSE 2019]{The 27th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering}{26--30 August, 2019}{Tallinn, Estonia}

%
% These commands are for a JOURNAL article.
%\setcopyright{acmcopyright}
%\acmJournal{TOG}
%\acmYear{2018}\acmVolume{37}\acmNumber{4}\acmArticle{111}\acmMonth{8}
%\acmDOI{10.1145/1122445.1122456}

%
% Submission ID. 
% Use this when submitting an article to a sponsored event. You'll receive a unique submission ID from the organizers
% of the event, and this ID should be used as the parameter to this command.
%\acmSubmissionID{123-A56-BU3}

%
% The majority of ACM publications use numbered citations and references. If you are preparing content for an event
% sponsored by ACM SIGGRAPH, you must use the "author year" style of citations and references. Uncommenting
% the next command will enable that style.
%\citestyle{acmauthoryear}



%
% end of the preamble, start of the body of the document source.
\begin{document}

%
% The "title" command has an optional parameter, allowing the author to define a "short title" to be used in page headers.
\title{Software Clusterings with Vector Semantics and 
the Call Graph}

%
% The "author" command and its associated commands are used to define the authors and their affiliations.
% Of note is the shared affiliation of the first two authors, and the "authornote" and "authornotemark" commands
% used to denote shared contribution to the research.
\author{Marios Papachristou}

\email{papachristoumarios@gmail.com}
\affiliation{%
  \institution{National Technical University of Athens}
  \streetaddress{Zografou Campus}
  \city{Athens}
  \state{Attica}
  \postcode{157 73}
}

%
% By default, the full list of authors will be used in the page headers. Often, this list is too long, and will overlap
% other information printed in the page headers. This command allows the author to define a more concise list
% of authors' names for this purpose.
\renewcommand{\shortauthors}{Papachristou}

\begin{abstract}

\noindent \textbf{Purpose -- } Propose a method to determine a software's modules without 
knowledge of its architectural structure, and empirically validate the method's performance.

\noindent \textbf{Method -- } Cluster files by combining document embeddings, generated with the  \emph{Doc2Vec} algorithm, and the call graph, provided by Static Graph Analyzers to an augmented graph.
Use of the \emph{Louvain Algorithm} to iteratively exhibit its community structure and propose a module--level clustering. 


\noindent \textbf{Results --} Our method performs better in terms of stability, authoritativeness 
and extremity over other state-of-the-art clustering methods proposed in literature and is able to decently 
recover the ground truth clustering of the Linux Kernel. 

\noindent \textbf{Conclusions --} Semantic information from vector semantics and the call graph can produce 
accurate results for software clusterings of large systems.

\end{abstract}

%
% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
% Please copy and paste the code instead of the example below.
%
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010147.10010257.10010258.10010260.10003697</concept_id>
<concept_desc>Computing methodologies~Cluster analysis</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010257.10010321</concept_id>
<concept_desc>Computing methodologies~Machine learning algorithms</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10011007.10010940.10010971.10010972</concept_id>
<concept_desc>Software and its engineering~Software architectures</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Cluster analysis}
\ccsdesc[500]{Computing methodologies~Machine learning algorithms}
\ccsdesc[500]{Software and its engineering~Software architectures}
%
% Keywords. The author(s) should pick words that accurately describe the work being
% presented. Separate the keywords with commas.
\keywords{document embeddings, doc2vec, linux kernel, natural language processing, software architecture recovery, software clustering, static graph analysis, vector semantics}



\maketitle

\section{Introduction}



%Modern software systems usually span hundreds of thousands of source code lines (SLOC) 
%since the inherent complexity of modern applications that computers have to solve has 
%exponentially increased. 

In modern software systems, the need for adoption of modular software architectural schemas 
becomes inevident for robust maintainability of the overall software system.
Since the architecture of a software system is the most fundamental realization of a 
software system, when there is no description of the architecture of it, software 
architects usually make attempts to recover it. In the past, there have been attempts to 
identify the main components of a software system using hierarchical clustering 
algorithms \cite{maqbool_overview, limbo} mainly by clustering modules into subsystems, given 
its source code.    
The possibilities offered by machine learning techniques and practices in the Natural Language Processing (NLP) 
can also be applied on codebases as corpuses of text, thus enabling 
the extraction of useful information for the semantics of the source codebase 
inaugurating new modes of reviewing large amounts of source code.
An example is demonstrated through the Deep Code Search Project \cite{deepcodesearch} .

This paper aims to propose software clusterings using a combination of vector semantics on the source code provided by \emph{Doc2Vec} as well as the existing structure of 
the codebase via the call graph in order to form communities adn arrive at a module-level 
architecture viewed from the call graph perspective. 
We will study Linux codebase and we will compare it with ground truth in using the MoJo metric \cite{mojo} as well as 
compare it with other state-of-the-art and baseline clustering algorithms \cite{maqbool_overview, evaluation} in terms of 
\emph{stability}, \emph{authoritativeness} and \emph{extremity}


% XXX Not sure if needed

% We begin with a brief outline of the theoretical background concerning layered 
% architectures addressing its conjunction with software analysis tools and NLP practices. 
% Next, we propose methods for analyzing large codebases using techniques outlined in the 
% Methodology section. Next, we present the case study of analyzing the Linux Kernel 
% codebase using certain evaluation criteria, as well as results induced by the methodology 
% we have proposed. Finally, conclude and recommend roadmaps for future research on the 
% topic. 



\section{Method} 

\begin{table*}
  \caption{Experimental Results for Linux 4.21 Codebase}
    \label{tab:evaluation}
    \begin{tabular}{lrrrrrr}
    \toprule
    Algorithm & Number of Clusters & Cluster Size Range & Avg. Cluster & Standard Deviation & Median Cluster & MoJo Distance \\
    \midrule
    ACDC \cite{acdc} & 9055 & 1 -- 4245 & 4.53 & 45.82 & 2\\
    Average Linkage \cite{average} & 21 & 1--3406 & 163 & 725.11 & 1 & 2092 \\
    Complete Linkage \cite{complete} & 21 & 1--1529 & 163 & 412.34 & 19.0 & 1710 \\
    LIMBO \cite{limbo} & & \\
    SADE & 10 (\pm 2)  & 2 (\pm 0) -132 (\pm 13) & 64 (\pm 4) & 40 (\pm 4) & 65 (\pm 10) & 243 (\pm 1)  \\
    SADE (Directed) & 5 (\pm 2) & 1 (\pm 1) - 614 (\pm 1) & 141 (\pm 39) & 253 (\pm 25) & 2 (\pm 0.3)  & 237 (\pm 2) \\
    
    Ward Linkage \cite{ward} & 21 & 21--948 & 163 & 223 & 70.0 & 1138 \\
    Ground Truth & 21 & 1--1348 & 163 & 340.62 & 11.0 & -- \\
    \bottomrule
  \end{tabular}
\end{table*}

\paragraph{Embeddings} 

For the preprocessing stage use the NLP package spaCy \cite{spacy} which provided us with very useful tools for the extraction. After preprocessing, we use gensim \cite{gensim} to train the Doc2Vec model. First of all, we tokenize the code and remove the stopwords. After that, for each token, we split it into its constituent parts using dynamic programming and lemmatize 
each individual sub-token. 
For example the method \texttt{\_\_zone\_seqlock\_init} corresponds to \texttt{zone}, \texttt{seqlock} and \texttt{init}, \texttt{inprogress} is split into \texttt{in} and \texttt{progress} and \texttt{literals} becomes \texttt{literal}.


% The identifier \texttt{inprogress} should be split into \texttt{in} and \texttt{progress} 
% which provides us with a more meaningful content for the word embeddings from a 
% linguistic point of view. The identifier segmentation problem is reduced to using an arbitrary number of separators we must split the identifier $s = i_1 i_2 \dots i_n$ such
% that the total cost of splitting such words minimizing the split cost. Using a word-level $n$-gram model we define the cost function of a (sub)-word $s_k$ given prior words $s_{k-1}, s_{k-2} \dots, s_{k-n + 1}$ and a dataset $\mathcal D$ as $c(s_k | s_{k-1} \dots s_{k - n + 1}, \mathcal D) = - \log \hat p_{MLE} (s_k | s_{k-1} \dots s_{k-n+1}, \mathcal D)$ where $\hat p_{MLE} (\cdot)$ is the Maximum Likelihood Estimator of the event. Let $L(m)$ be the cost of the optimal split in the $m$ character of the identifier. Then 

% $$L(m) = 
% \begin{cases}
% 	\min_{0 \le \ell < m} \{ L(\ell) + c(s[\ell:m]  \mid \\ \text{ prior words to } s% [\ell:m] ) \} & 1 \le m \le |s| \\
%	0 & m = 0

%\end{cases}
%$$

\paragraph{Call Graph Generation} The codebase is processed by a Static Graph Analyzer. In our approach, focusing on C projects, we use \emph{CScout} \cite{cscout} in order to generate the directed call graph. Each source code file is assigned to a module. Modules can be user defined or automatically generated by their respective directories at a desired directory tree depth. 

\paragraph{Combining semantics and the Call Graph} The input to the clustering procedure
 is the call graph with cosine similarities of the various modules as weights, thus forming a
  weighted directed graph. Then we run the Louvain Clustering Algorithm \cite{louvain} in
   order to obtain the software clustering via maximizing the modularity function with a
    greedy approach. 
    
    %Further refining can also be done by re-running the algorithm on the contracted graph with mean embeddings as embeddings for each node.  

\paragraph{Implementation}  Our method is implemented in the Python Language using spaCy\cite{spacy} Gensim \cite{gensim} and NetworkX \cite{nx}.

% used to handle graphs and graph operations was NetworkX \cite{nx}. The code is available on GitHub.

\paragraph{Decision Rationale for Evaluation System} We have chosen to use Linux as a codebase to evaluate our method on since it is a large and complex system with 27 years of continuous development, spanning 20.3 millions SLOC at the time of writing.  Moreover, it is easy to find ground truth system due to its clear directory structure and construct testcases for
evaluations. Finally, it was also used in related studies \cite{acdc, evaluation} as a reference system for evaluation. 

\paragraph{Experimental Setting} Our method was run on Linux 4.21,
 consisting of 20.3 million SLOC against Average-Linkage \cite{average}, Complete-Linkage \cite{complete} and Ward-Linkage \cite{ward} as well as ACDC and LIMBO. The results are presented in Table \ref{tab:evaluation}. As ground truth we have used the first level directories as a target clustering and as input we have considered the modules of the one-top directories. For example, the source code file \texttt{drivers/net/ieee802154/mcr20a.c} has a ground truth value of \texttt{drivers} and it is considered under the same module as every \texttt{.c} and \texttt{.h} file under \texttt{drivers/net/ieee802154}. As a clustering distance metric we have used the MoJo metric, as proposed in reference \cite{mojo}. Since, Louvain Community Detection produces 
 different (but very similar) results every time it runs, we ran the simulations multiple times and averaged the results. 

\section{Results, Evaluation, Discussion}

 In Table \ref{tab:evaluation}, there are the results of the clusterings ordered in increasing MoJo distance with respect to the Ground Truth. We have included the number of clusters produced, the cluster size range, the average cluster size the standard deviation in the cluster sizes, the median cluster and the MoJo distances with respect to the ground truth. The Ground Truth clustering is also present in the table. Our quality metrics for the clusterings are 
\emph{extremity}, \emph{authoritativeness} and \emph{stability} as proposed in reference \cite{evaluation}

Firstly, our approach produced balanced clusterings thus demonstrating low \emph{extremity} compared to the other clusterings, producing an amount of clusters that was close to the Ground Truth with smaller standard deviation in the cluster sizes than the other clustering methods, without knowing the number of clusters \emph{a priori}. Besides this, our approach also seems to give balanced clusters with respect to \emph{Median Cluster} criterion. Note that ACDC, which took into account only the structural properties of the files, produced a high granularity clustering that failed to catch the structure of Linux Codebase. 
Moreover, in the \emph{authoritativeness} criterion, our approach outperformed the other clusterings to a very up to 10 times in terms of MoJo distance being very close (smaller is better) to the Ground Truth, with a very small standard deviation. This also suggests great
\emph{stability} in the resulting clusterings. Note that the other clustering methods we compared against ours gave disappointing MoJo distances even with their cluster number defined 
equal to the one of the Ground Truth. 


\section{Related Work}

% TODO Needs review

% The architecture of a software system is a \emph{"living organism"} meaning that it is 
% changing constantly over time, even for small projects. The evolution software 
% architecture may lead to architectural decay \cite{large_study}. The identification of such 
% a decay is non-trivial and requires a large amount of effort and understanding of the 
% software system in order to be diagnosed. 

% Architectural recovery is an active research 
%field aiming to make software systems more understandable and maintainable over time. The 
%study of architectural change requires that the architecture at a given point in time, 
%for example after a major release, during a system's evolution must be inferred. 
%Various algorithms have been proposed for realizing architectural schemata many of which use 
%different features for describing the source code. 

Onaiza Maqbool and Haroon A. Babri \cite{maqbool_overview} present an
overview of the various approaches taken towards to hierarchical clustering algorithms
for software systems. In their study, they compare the various clustering algorithms
such as ACDC, LIMBO and other Traditional Clustering Algorithms such as Single
Linkage, Complete Linkage \cite{complete} and Weighted Linkage presenting their efficiency regarding 
multiple feature sets. 
Added to this, a \emph{semantics-based architectural view} of the system, as discussed in reference
\cite{large_study} reveals significant aspects of a software system and its change over 
time, which suggests that semantic-based approached should be followed for gaining better 
understandings of a software system. Their suggestion is a \emph{term frequency--inverse 
document frequency} (TF-IDF) model serving as a similarity metric between source code 
entities. 
Work in Software Clustering Algorithms (SCA) has been extensively done by Tzerpos in
references \cite{mojo}, \cite{stability}, \cite{acdc} and \cite{limbo} with the ACDC
and LIMBO clustering algorithms. 

In particular, the ACDC algorithm leans toward to software components comprehension 
based on subsystem patterns. Their approach considers an initial structure of the system, without taking into account semantics, and tries to build comprehensive clusterings of the given ground structure. The authors also conducted a study on an older version of  Linux. 
Moreover, Andritsos and Tzerpos in reference \cite{limbo} present an Information--Theoretic 
approach of SCA by developing LIMBO which clusters modules upon inserting their Distributional Cluster Features to a B+-tree variant and then applying the \emph{Agglomerative Information Bottleneck} algorithm, merging the clusters with the minimum mutual information loss together. 


\section{Conclusions} This paper is set out to show how we can use a combination of vector semantics
and information from the call graph in order to produce meaningful clusterings of a software system.
We conclude that we have outperformed state-of-the-art software clusterings and conventional
agglomerative clustering algorithms 
in terms of \emph{extremity}, \emph{authoritativeness} and \emph{stability} 
without knowing the number of clusters of the Ground Truth. This evidence strongly
supports the further usage of vector semantics and the call graph for architecture recovery.
Finally, further work is encouraged in several areas such as 
integration with more static analyzers and use of the project in order to propose layered architectures.











\newpage
\bibliographystyle{ACM-Reference-Format}
\bibliography{references}



\end{document}
